{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphQL\n",
    "from graphql_helper import run_query\n",
    "from gql import gql, Client\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "# I/O\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# statistical analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MaxNLocator\n",
    "import pwlf_helper\n",
    "\n",
    "import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniswap_v3_subgraph_url = \"https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3\"\n",
    "refetch = True\n",
    "data_folder = \"data\"\n",
    "pool_day_data_folder = os.path.join(data_folder, \"pool\", \"day\")\n",
    "\n",
    "# TODO: whitelist by address, not symbols\n",
    "whitelisted_symbols = [\"BUSD\", \"USDC\", \"USDT\", \"DAI\", \"WETH\", \"WBNB\", \"WBTC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEN_LARGEST_TVL_POOLS_QUERY = \"\"\"\n",
    "{\n",
    "    pools(first: 10, orderBy: totalValueLockedUSD, orderDirection: desc) {\n",
    "        id\n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "if refetch:\n",
    "    largest_tvl_pool_ids = run_query(uniswap_v3_subgraph_url, TEN_LARGEST_TVL_POOLS_QUERY)\n",
    "    largest_tvl_pool_addrs = list(map(lambda x: x[\"id\"], largest_tvl_pool_ids[\"data\"][\"pools\"]))\n",
    "    print(largest_tvl_pool_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_POOL_BY_ID_QUERY = gql(\n",
    "    \"\"\"\n",
    "    query getPoolById($pool_addr: ID!) {\n",
    "        pool(id: $pool_addr) {\n",
    "            tick\n",
    "            token0 {\n",
    "                symbol\n",
    "                id\n",
    "                decimals\n",
    "            }\n",
    "            token1 {\n",
    "                symbol\n",
    "                id\n",
    "                decimals\n",
    "            }\n",
    "            feeTier\n",
    "            sqrtPrice\n",
    "            liquidity\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_pools_metadata(addresses: list, verbose: bool=False):\n",
    "    result = {\"topPoolDatas\": []}\n",
    "\n",
    "    transport = AIOHTTPTransport(url=uniswap_v3_subgraph_url)\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "    ) as session:\n",
    "        for id in addresses:\n",
    "            params = {\"pool_addr\": id}\n",
    "            temp = await session.execute(GET_POOL_BY_ID_QUERY, variable_values=params)\n",
    "            result[\"topPoolDatas\"].append(temp[\"pool\"])\n",
    "\n",
    "    if verbose:\n",
    "        pprint(result)\n",
    "\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    with open(f\"{data_folder}/topPoolDatas.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refetch:\n",
    "    await fetch_pools_metadata(largest_tvl_pool_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pool_name(symbol0: str, symbol1: str, fee_tier: int) -> str:\n",
    "    return f\"{symbol0}_{symbol1}_{fee_tier}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_folder}/topPoolDatas.json\", \"r\") as f:\n",
    "    top_pool_datas = json.load(f)\n",
    "\n",
    "# TODO: better token symbol mapping\n",
    "token_symbols = dict()\n",
    "\n",
    "# replace nested dict with token addr\n",
    "for pool_dict in top_pool_datas[\"topPoolDatas\"]:\n",
    "    for token in [\"token0\", \"token1\"]:\n",
    "        token_symbols[pool_dict[token][\"id\"]] = pool_dict[token][\"symbol\"]\n",
    "        pool_dict[token] = pool_dict[token][\"id\"]\n",
    "\n",
    "top_pools_df = pd.DataFrame.from_dict(top_pool_datas[\"topPoolDatas\"]).astype({\n",
    "    \"token0\": str,\n",
    "    \"token1\": str,\n",
    "    \"feeTier\": int\n",
    "})\n",
    "\n",
    "# add addr and name\n",
    "top_pools_df[\"pool_addr\"] = largest_tvl_pool_addrs\n",
    "top_pools_df[\"name\"] = top_pools_df.apply(lambda x: format_pool_name(token_symbols[x[\"token0\"]], token_symbols[x[\"token1\"]], x[\"feeTier\"]), axis=1)\n",
    "\n",
    "# drop unused cols\n",
    "top_pools_df.drop(columns=[\"tick\", \"sqrtPrice\", \"liquidity\"], inplace=True)\n",
    "\n",
    "# drop pools without whitelisted symbols\n",
    "has_whitelisted_symbols = top_pools_df[\"name\"].str.contains(\"|\".join(whitelisted_symbols))\n",
    "top_pools_df = top_pools_df[has_whitelisted_symbols]\n",
    "\n",
    "top_pools_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pools_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating the Info Page\n",
    "We perform the following two queries over 1000 days (at most):\n",
    "1. TVL\n",
    "2. 24H Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_CHART = gql(\n",
    "    \"\"\"\n",
    "    query poolDayDatas($startTime: Int!, $skip: Int!, $address: String!) {\n",
    "        poolDayDatas(\n",
    "            first: 1000\n",
    "            skip: $skip\n",
    "            where: { pool: $address, date_gt: $startTime }\n",
    "            orderBy: date\n",
    "            orderDirection: asc\n",
    "            subgraphError: allow\n",
    "        ) {\n",
    "            date\n",
    "            volumeUSD\n",
    "            tvlUSD\n",
    "            feesUSD\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_pool_error = []\n",
    "\n",
    "# ref: https://github.com/Uniswap/v3-info/blob/770a05dc1a191cf229432ebc43c1f2ceb3666e3b/src/data/pools/chartData.ts#L14\n",
    "async def fetch_pool_chart_data(address: str, symbol0: str, symbol1: str, fee_tier: int, verbose: bool=False):\n",
    "    START_TIMESTAMP = 1619170975 # GMT: Friday, April 23, 2021 9:42:55 AM\n",
    "    # END_TIMESTAMP = int(time.time()) # current timestamp\n",
    "\n",
    "    error = False\n",
    "    skip = 0\n",
    "    all_found = False\n",
    "    result = {\"poolDayDatas\": []}\n",
    "\n",
    "    transport = AIOHTTPTransport(url=uniswap_v3_subgraph_url)\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "    ) as session:\n",
    "        params = {\n",
    "            \"address\": address,\n",
    "            \"startTime\": START_TIMESTAMP,\n",
    "            \"skip\": skip\n",
    "        }\n",
    "        try:\n",
    "            while not all_found:\n",
    "                temp = await session.execute(POOL_CHART, variable_values=params)\n",
    "                skip += 1000\n",
    "                if len(temp[\"poolDayDatas\"]) < 1000 or error:\n",
    "                    all_found = True\n",
    "                if temp:\n",
    "                    result[\"poolDayDatas\"] = result[\"poolDayDatas\"] + temp[\"poolDayDatas\"] # concat the lists\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error = True\n",
    "            fetch_pool_error.append(address)\n",
    "\n",
    "    if not error:\n",
    "        if verbose:\n",
    "            pprint(result)\n",
    "\n",
    "        if not os.path.exists(pool_day_data_folder):\n",
    "            os.makedirs(pool_day_data_folder)\n",
    "\n",
    "        with open(f\"{pool_day_data_folder}/{format_pool_name(symbol0, symbol1, fee_tier)}.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refetch:\n",
    "    # remove existing content in the out folder\n",
    "    for f in glob.glob(pool_day_data_folder + \"/*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    # fetch pool data for each pool\n",
    "    for i, row in top_pools_df.iterrows():\n",
    "        await fetch_pool_chart_data(row[\"pool_addr\"], token_symbols[row[\"token0\"]], token_symbols[row[\"token1\"]], row[\"feeTier\"])\n",
    "    print(fetch_pool_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads pool day datas from json\n",
    "df = pd.DataFrame(columns=[\"date\"])\n",
    "pool_names = []\n",
    "\n",
    "for f in os.listdir(pool_day_data_folder):\n",
    "    fullname = os.fsdecode(f)\n",
    "\n",
    "    # not a rigorous check\n",
    "    with open(os.path.join(pool_day_data_folder, fullname), \"r\") as file:\n",
    "        pool_day_datas = json.load(file)\n",
    "\n",
    "    # parse dict as df\n",
    "    temp = pd.DataFrame.from_dict(pool_day_datas[\"poolDayDatas\"]).astype({\n",
    "        \"volumeUSD\": np.float64,\n",
    "        \"tvlUSD\": np.float64\n",
    "    })\n",
    "\n",
    "    # Note: there is no need to analyze fees separately,\n",
    "    # as it is a fixed proportion of the pool's trade volume\n",
    "    temp.drop(columns=[\"feesUSD\"], inplace=True)\n",
    "\n",
    "    # prefix columns (except \"date\") with pool name\n",
    "    cols = temp.columns[~temp.columns.isin([\"date\"])]\n",
    "    pool_name = fullname.split(os.sep)[-1].split(\".\")[0]\n",
    "    pool_names.append(pool_name)\n",
    "    temp.rename(columns = dict(zip(cols, pool_name + \"_\" + cols)), inplace=True)\n",
    "\n",
    "    # outer join: union of items on \"date\"\n",
    "    df = pd.merge(df, temp, how=\"outer\", on=[\"date\"])\n",
    "\n",
    "# sort by \"date\"\n",
    "df.sort_values(by=\"date\", inplace=True)\n",
    "df.reset_index(drop=\"index\", inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"date\"]: int -> date (in \"YYYY-MM-DD\")\n",
    "df[\"timestamp\"] = df[\"date\"] # keep timestamp in a new col\n",
    "df[\"date\"] = df[\"date\"].map(dt.date.fromtimestamp)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for number of days elapsed\n",
    "print(df[\"date\"][0], \"to\", dt.date.today(), \"has\", (dt.date.today() - df[\"date\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24H Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19, 9))\n",
    "for pool_name in pool_names:\n",
    "    plt.plot(df[\"date\"], df[pool_name + \"_volumeUSD\"])\n",
    "plt.title(\"24H Volume over Time\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"24H Volume (in USD)\")\n",
    "plt.legend(pool_names, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the days with the greatest 24H volumes\n",
    "# df.sort_values(by=\"volumeUSD\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for reference only, no use now\n",
    "# fig, ax = plt.subplots(figsize=(15, 1))\n",
    "# sns.boxplot(data=df, x=\"volumeUSD\")\n",
    "# plt.xlim(0, 2e8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "Observe the distribution of the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "axes = fig.subplots(3, 3)\n",
    "for i, pool_name in enumerate(pool_names):\n",
    "    volumeUSD_series = df[pool_name + \"_volumeUSD\"]\n",
    "    ax = axes[math.floor(i/3), i%3]\n",
    "    ax.title.set_text(pool_name)\n",
    "    ax.hist(volumeUSD_series, bins=100, range=(0, volumeUSD_series.quantile(0.99)))\n",
    "\n",
    "    # force y-axis ticks to use integers\n",
    "    ax.get_yaxis().set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # highlight 25%-75% percentile\n",
    "    lq = volumeUSD_series.quantile(0.25)\n",
    "    uq = volumeUSD_series.quantile(0.75)\n",
    "    ax.axvspan(lq, uq, color=\"green\", alpha=0.25)\n",
    "\n",
    "fig.suptitle(\"24H Volume Distributions\")\n",
    "fig.supxlabel(\"24H Volume (in USD)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_df = df.drop(columns=\"timestamp\")\n",
    "# note: df.std() is normalized by N-1\n",
    "pool_metrics_df = pd.DataFrame(data=[pool_df.mean(), pool_df.std()], index=[\"mean\", \"stdev\"])\n",
    "pool_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for reference only, no use now\n",
    "# df[\"volumeUSD\"].plot.kde()\n",
    "# plt.title(pool_name + \" 24H Volume KDE\")\n",
    "# plt.xlim(0, 2e8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: time series / autocorelation\n",
    "# TODO: aggregate weekly and daily patterns and look for anomalies (e.g. Friday)\n",
    "# TODO: ask for calculation of Greeks (Detla, Vega...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fourier Transform (FFT)\n",
    "FFT computes the frequency content of the prices as signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "axes = fig.subplots(3, 3)\n",
    "for i, pool_name in enumerate(pool_names):\n",
    "    date_volume_df = df[[\"date\", pool_name + \"_volumeUSD\"]].dropna()\n",
    "    volumeUSD_series = date_volume_df[pool_name + \"_volumeUSD\"]\n",
    "\n",
    "    # reference for zero-mean signal:\n",
    "    # https://dsp.stackexchange.com/questions/46950/removing-mean-from-signal-massively-distorts-fft\n",
    "    # only keep those with freq STRICTLY > 0\n",
    "    f_max = math.ceil(date_volume_df.shape[0]/2)\n",
    "    Y = abs(np.fft.fft(volumeUSD_series - volumeUSD_series.mean()))[1:f_max]\n",
    "    freq = np.fft.fftfreq(date_volume_df.shape[0], 1)[1:f_max]\n",
    "\n",
    "    ax = axes[math.floor(i/3), i%3]\n",
    "    ax.title.set_text(pool_name)\n",
    "    ax.plot(freq, Y)\n",
    "\n",
    "fig.suptitle(\"24H Volume FFT\")\n",
    "fig.supxlabel(\"freq (in /day)\")\n",
    "fig.supylabel(\"24H Volume (in USD)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "axes = fig.subplots(3, 3)\n",
    "for i, pool_name in enumerate(pool_names):\n",
    "    date_volume_df = df[[\"date\", pool_name + \"_volumeUSD\"]].dropna()\n",
    "    volumeUSD_series = date_volume_df[pool_name + \"_volumeUSD\"]\n",
    "\n",
    "    # reference for zero-mean signal:\n",
    "    # https://dsp.stackexchange.com/questions/46950/removing-mean-from-signal-massively-distorts-fft\n",
    "    # only keep those with freq STRICTLY > 0\n",
    "    f_max = math.ceil(date_volume_df.shape[0]/2)\n",
    "    Y = abs(np.fft.fft(volumeUSD_series - volumeUSD_series.mean()))[1:f_max]\n",
    "    freq = np.fft.fftfreq(date_volume_df.shape[0], 1)[1:f_max]\n",
    "\n",
    "    # c.f. power spectral density in signal processing\n",
    "    spectrum = Y.real*Y.real + Y.imag*Y.imag\n",
    "\n",
    "    ax = axes[math.floor(i/3), i%3]\n",
    "    ax.title.set_text(pool_name)\n",
    "    ax.set_xlim(left=freq[1], right=freq[-1])\n",
    "\n",
    "    # Note: this is possible because FFT must give positive values,\n",
    "    # so that their logarithms always exist.\n",
    "    # plot log10(spectrum) against frequency\n",
    "    ax.semilogy(freq, spectrum)\n",
    "\n",
    "fig.suptitle(\"Semilog Plot of 24H Volume FFT\")\n",
    "fig.supxlabel(\"freq (in /day)\")\n",
    "fig.supylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "axes = fig.subplots(3, 3, sharex=True)\n",
    "for i, pool_name in enumerate(pool_names):\n",
    "    date_tvl_df = df[[\"date\", pool_name + \"_tvlUSD\", \"timestamp\"]].dropna()\n",
    "    # TODO: customize number of turning points\n",
    "    reg_result = pwlf_helper.regression(date_tvl_df[\"timestamp\"], date_tvl_df[pool_name + \"_tvlUSD\"], 6)\n",
    "\n",
    "    ax = axes[math.floor(i/3), i%3]\n",
    "    ax.title.set_text(pool_name + \" TVL over Time\")\n",
    "\n",
    "    ax.plot(date_tvl_df[\"date\"], date_tvl_df[pool_name + \"_tvlUSD\"])\n",
    "    ax.plot(date_tvl_df[\"date\"], reg_result.yHat, '--')\n",
    "\n",
    "    # ax.legend([\"TVL\", \"PWLF fitted trend line\"])\n",
    "\n",
    "    # # x-axis ticks are spaced out biweekly (for now)\n",
    "    # ax.xticks(pd.date_range(date_tvl_df[\"date\"].iloc[0], date_tvl_df[\"date\"].iloc[-1], freq=\"14D\"))\n",
    "\n",
    "    # # annotate turning points\n",
    "    # for tp in reg_result.tp[1:-1]:\n",
    "    #     tp_date = dt.date.fromtimestamp(tp)\n",
    "    #     tp_str = tp_date.strftime(\"%Y-%m-%d\")\n",
    "    #     pred = reg_result.predict(tp)\n",
    "    #     ax.annotate(tp_str, xy=(tp_date, pred), xytext=(tp_date, pred+0.5e8),\n",
    "    #         arrowprops=dict(arrowstyle=\"->\", color='red')\n",
    "    # )\n",
    "\n",
    "fig.suptitle(\"Growth Stages of TVL\")\n",
    "fig.supxlabel(\"date\")\n",
    "fig.supylabel(\"TVL (in USD)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
