{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we fetch data from Uniswap V3 subgraph, and store them in json for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import datetime as dt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Third Party Library\n",
    "import numpy as np\n",
    "from flatdict import FlatDict\n",
    "import pandas as pd\n",
    "from gql import Client, gql\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "# Local Folder Library\n",
    "from pyammanalysis.graphql_helper import run_query\n",
    "from pyammanalysis.util import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refetch setting - if True, rerun GraphQL queries\n",
    "refetch = True\n",
    "\n",
    "# data folder paths\n",
    "data_folder = \"data\"\n",
    "pool_day_data_folder = os.path.join(data_folder, \"pool\", \"day\")\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "# config\n",
    "config = read_yaml(\"../config.yaml\")\n",
    "UNISWAP_V3_SUBGRAPH_URL = config[\"UNISWAP_V3_SUBGRAPH_URL\"]\n",
    "\n",
    "token_dict = config[\"tokens\"]\n",
    "token_addr_dict = config[\"token_addr\"]  # mapping from symbol to addr\n",
    "whitelisted_symbols = np.sort(\n",
    "    np.concatenate([i for i in FlatDict(token_dict).itervalues()])\n",
    ")\n",
    "\n",
    "# address-related config\n",
    "# addresses in `config.yaml` follow EIP-55: Mixed-case checksum address encoding\n",
    "# enforce lower case by `str.lower()`\n",
    "sym2addr = lambda symbol: config[\"token_addr\"][\n",
    "    symbol\n",
    "].lower()  # mapping from symbol to addr\n",
    "addr2sym = lambda addr: {v.lower(): k for k, v in config[\"token_addr\"].items()}[\n",
    "    addr\n",
    "]  # mapping from addr to symbol\n",
    "whitelisted_addresses = np.array(\n",
    "    [i.lower() for i in FlatDict(token_addr_dict).itervalues()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_LARGEST_TVL_POOLS_QUERY = \"\"\"\n",
    "{\n",
    "    pools(first: 20, orderBy: totalValueLockedUSD, orderDirection: desc) {\n",
    "        id\n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "LARGEST_TVL_POOLS_PATH = os.path.join(data_folder, \"largestTVLPoolAddr.json\")\n",
    "\n",
    "if refetch:\n",
    "    largest_tvl_pool_ids = run_query(\n",
    "        UNISWAP_V3_SUBGRAPH_URL, TWENTY_LARGEST_TVL_POOLS_QUERY\n",
    "    )\n",
    "    largest_tvl_pool_addrs = list(\n",
    "        map(lambda x: x[\"id\"], largest_tvl_pool_ids[\"data\"][\"pools\"])\n",
    "    )\n",
    "    with open(LARGEST_TVL_POOLS_PATH, \"w\") as f:\n",
    "        json.dump({\"poolList\": largest_tvl_pool_addrs}, f, indent=4)\n",
    "else:\n",
    "    with open(LARGEST_TVL_POOLS_PATH, \"r\") as f:\n",
    "        largest_tvl_pool_addrs = json.load(f)[\"poolList\"]\n",
    "\n",
    "print(largest_tvl_pool_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_POOL_BY_ID_QUERY = gql(\n",
    "    \"\"\"\n",
    "    query getPoolById($pool_addr: ID!) {\n",
    "        pool(id: $pool_addr) {\n",
    "            tick\n",
    "            token0 {\n",
    "                symbol\n",
    "                id\n",
    "                decimals\n",
    "            }\n",
    "            token1 {\n",
    "                symbol\n",
    "                id\n",
    "                decimals\n",
    "            }\n",
    "            feeTier\n",
    "            sqrtPrice\n",
    "            liquidity\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_pools_metadata(addresses: list, verbose: bool = False):\n",
    "    result = {\"topPoolDatas\": []}\n",
    "\n",
    "    transport = AIOHTTPTransport(url=UNISWAP_V3_SUBGRAPH_URL)\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "    ) as session:\n",
    "        for id in addresses:\n",
    "            params = {\"pool_addr\": id}\n",
    "            temp = await session.execute(GET_POOL_BY_ID_QUERY, variable_values=params)\n",
    "            result[\"topPoolDatas\"].append(temp[\"pool\"])\n",
    "\n",
    "    if verbose:\n",
    "        pprint(result)\n",
    "\n",
    "    with open(f\"{data_folder}/topPoolDatas.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refetch:\n",
    "    await fetch_pools_metadata(largest_tvl_pool_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pool_name(symbol0: str, symbol1: str, fee_tier: int) -> str:\n",
    "    return f\"{symbol0}_{symbol1}_{fee_tier}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_folder}/topPoolDatas.json\", \"r\") as f:\n",
    "    top_pool_datas = json.load(f)\n",
    "\n",
    "# replace nested dict with token addr\n",
    "for pool_dict in top_pool_datas[\"topPoolDatas\"]:\n",
    "    for token in [\"token0\", \"token1\"]:\n",
    "        pool_dict[token] = pool_dict[token][\"id\"]\n",
    "\n",
    "top_pools_df = pd.DataFrame.from_dict(top_pool_datas[\"topPoolDatas\"]).astype(\n",
    "    {\"token0\": str, \"token1\": str, \"feeTier\": int}\n",
    ")\n",
    "\n",
    "# drop unused cols\n",
    "top_pools_df.drop(columns=[\"tick\", \"sqrtPrice\", \"liquidity\"], inplace=True)\n",
    "\n",
    "# add addr\n",
    "top_pools_df[\"pool_addr\"] = largest_tvl_pool_addrs\n",
    "\n",
    "# whitelist a pool if both its token0 and token1 are whitelisted\n",
    "is_whitelisted_pool = top_pools_df[\"token0\"].isin(whitelisted_addresses) & top_pools_df[\n",
    "    \"token1\"\n",
    "].isin(whitelisted_addresses)\n",
    "top_pools_df = top_pools_df[is_whitelisted_pool]\n",
    "\n",
    "# add name\n",
    "top_pools_df[\"name\"] = top_pools_df.apply(\n",
    "    lambda x: format_pool_name(\n",
    "        addr2sym(x[\"token0\"]), addr2sym(x[\"token1\"]), x[\"feeTier\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "top_pools_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pools_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_CHART = gql(\n",
    "    \"\"\"\n",
    "    query poolDayDatas($startTime: Int!, $skip: Int!, $address: String!) {\n",
    "        poolDayDatas(\n",
    "            first: 1000\n",
    "            skip: $skip\n",
    "            where: { pool: $address, date_gt: $startTime }\n",
    "            orderBy: date\n",
    "            orderDirection: asc\n",
    "            subgraphError: allow\n",
    "        ) {\n",
    "            date\n",
    "            volumeUSD\n",
    "            tvlUSD\n",
    "            feesUSD\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_pool_error = []\n",
    "\n",
    "# ref: https://github.com/Uniswap/v3-info/blob/770a05dc1a191cf229432ebc43c1f2ceb3666e3b/src/data/pools/chartData.ts#L14\n",
    "async def fetch_pool_chart_data(\n",
    "    address: str, symbol0: str, symbol1: str, fee_tier: int, verbose: bool = False\n",
    "):\n",
    "    START_TIMESTAMP = 1619170975  # GMT: Friday, April 23, 2021 9:42:55 AM\n",
    "    # END_TIMESTAMP = int(time.time()) # current timestamp\n",
    "\n",
    "    error = False\n",
    "    skip = 0\n",
    "    all_found = False\n",
    "    result = {\"poolDayDatas\": []}\n",
    "\n",
    "    transport = AIOHTTPTransport(url=UNISWAP_V3_SUBGRAPH_URL)\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "    ) as session:\n",
    "        params = {\"address\": address, \"startTime\": START_TIMESTAMP, \"skip\": skip}\n",
    "        try:\n",
    "            while not all_found:\n",
    "                temp = await session.execute(POOL_CHART, variable_values=params)\n",
    "                skip += 1000\n",
    "                if len(temp[\"poolDayDatas\"]) < 1000 or error:\n",
    "                    all_found = True\n",
    "                if temp:\n",
    "                    result[\"poolDayDatas\"] = (\n",
    "                        result[\"poolDayDatas\"] + temp[\"poolDayDatas\"]\n",
    "                    )  # concat the lists\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error = True\n",
    "            fetch_pool_error.append(address)\n",
    "\n",
    "    if not error:\n",
    "        if verbose:\n",
    "            pprint(result)\n",
    "\n",
    "        if not os.path.exists(pool_day_data_folder):\n",
    "            os.makedirs(pool_day_data_folder)\n",
    "\n",
    "        with open(\n",
    "            f\"{pool_day_data_folder}/{format_pool_name(symbol0, symbol1, fee_tier)}.json\",\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refetch:\n",
    "    # remove existing content in the out folder\n",
    "    for f in glob.glob(pool_day_data_folder + \"/*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    # fetch pool data for each pool\n",
    "    for i, row in top_pools_df.iterrows():\n",
    "        await fetch_pool_chart_data(\n",
    "            row[\"pool_addr\"],\n",
    "            addr2sym(row[\"token0\"]),\n",
    "            addr2sym(row[\"token1\"]),\n",
    "            row[\"feeTier\"],\n",
    "        )\n",
    "    print(fetch_pool_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads pool day datas from json\n",
    "df = pd.DataFrame(columns=[\"date\"])\n",
    "pool_names = []\n",
    "\n",
    "for f in os.listdir(pool_day_data_folder):\n",
    "    fullname = os.fsdecode(f)\n",
    "\n",
    "    # not a rigorous check\n",
    "    with open(os.path.join(pool_day_data_folder, fullname), \"r\") as file:\n",
    "        pool_day_datas = json.load(file)\n",
    "\n",
    "    # parse dict as df\n",
    "    temp = pd.DataFrame.from_dict(pool_day_datas[\"poolDayDatas\"]).astype(\n",
    "        {\"volumeUSD\": np.float64, \"tvlUSD\": np.float64}\n",
    "    )\n",
    "\n",
    "    # Note: there is no need to analyze fees separately,\n",
    "    # as it is a fixed proportion of the pool's trade volume\n",
    "    temp.drop(columns=[\"feesUSD\"], inplace=True)\n",
    "\n",
    "    # prefix columns (except \"date\") with pool name\n",
    "    cols = temp.columns[~temp.columns.isin([\"date\"])]\n",
    "    pool_name = fullname.split(os.sep)[-1].split(\".\")[0]\n",
    "    pool_names.append(pool_name)\n",
    "    temp.rename(columns=dict(zip(cols, pool_name + \"_\" + cols)), inplace=True)\n",
    "\n",
    "    # outer join: union of items on \"date\"\n",
    "    df = pd.merge(df, temp, how=\"outer\", on=[\"date\"])\n",
    "\n",
    "# sort by \"date\"\n",
    "df.sort_values(by=\"date\", inplace=True)\n",
    "df.reset_index(drop=\"index\", inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"date\"]: int -> date (in \"YYYY-MM-DD\")\n",
    "df[\"timestamp\"] = df[\"date\"]  # keep timestamp in a new col\n",
    "df[\"date\"] = df[\"date\"].map(dt.date.fromtimestamp)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for number of days elapsed\n",
    "print(df[\"date\"][0], \"to\", dt.date.today(), \"has\", (dt.date.today() - df[\"date\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_df = pd.DataFrame(\n",
    "    list(zip(pool_names, largest_tvl_pool_addrs)), columns=[\"name\", \"addr\"]\n",
    ")\n",
    "pools_df.to_csv(os.path.join(data_folder, \"pools_df.csv\"), index=False)\n",
    "df.to_csv(os.path.join(pool_day_data_folder, \"poolDay.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('eth-uniswap-prelim-analysis-3WCyaTCY-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c4b7809061330062f8c735d3b8d55bf6a8b663fb533c57ef3c9775113b0f4a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
