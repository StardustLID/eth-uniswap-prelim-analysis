{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we fetch data from Uniswap V3 subgraph, and store them in json for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import datetime as dt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Third Party Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flatdict import FlatDict\n",
    "from gql import Client, gql\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "# Local Folder Library\n",
    "from pyammanalysis.graphql_helper import run_query\n",
    "from pyammanalysis.util import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refetch setting - if True, rerun GraphQL queries\n",
    "refetch = True\n",
    "\n",
    "# config\n",
    "config = read_yaml(\"../config.yaml\")\n",
    "DATA_PATH = config[\"DATA_PATH\"]\n",
    "DATA_TOKEN_DAY_PATH = os.path.join(DATA_PATH, \"token\", \"day\")\n",
    "DATA_POOL_DAY_PATH = os.path.join(DATA_PATH, \"pool\", \"day\")\n",
    "UNISWAP_V3_SUBGRAPH_URL = config[\"UNISWAP_V3_SUBGRAPH_URL\"]\n",
    "\n",
    "if refetch:\n",
    "    transport = AIOHTTPTransport(url=UNISWAP_V3_SUBGRAPH_URL)\n",
    "\n",
    "# start timestamp for time series\n",
    "START_TIMESTAMP = 1619170975  # GMT: Friday, April 23, 2021 9:42:55 AM\n",
    "\n",
    "# create folder if needed\n",
    "for folder in [DATA_PATH, DATA_TOKEN_DAY_PATH, DATA_POOL_DAY_PATH]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "token_dict = config[\"tokens\"]\n",
    "token_addr_dict = config[\"token_addr\"]  # mapping from symbol to addr\n",
    "whitelisted_symbols = np.sort(\n",
    "    np.concatenate([i for i in FlatDict(token_dict).itervalues()])\n",
    ")\n",
    "\n",
    "# address-related config\n",
    "# addresses in `config.yaml` follow EIP-55: Mixed-case checksum address encoding\n",
    "# enforce lower case by `str.lower()`\n",
    "sym2addr = lambda symbol: config[\"token_addr\"][\n",
    "    symbol\n",
    "].lower()  # mapping from symbol to addr\n",
    "addr2sym = lambda addr: {v.lower(): k for k, v in config[\"token_addr\"].items()}[\n",
    "    addr\n",
    "]  # mapping from addr to symbol\n",
    "whitelisted_addresses = np.array(\n",
    "    [i.lower() for i in FlatDict(token_addr_dict).itervalues()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniswap V3 Global Data\n",
    "For now we only fetch the newest pool count and TVL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_GLOBAL_DATA_QUERY = \"\"\"\n",
    "{\n",
    "    factory(id: \"0x1F98431c8aD98523631AE4a59f267346ea31F984\" ) {\n",
    "        poolCount\n",
    "        totalValueLockedUSD\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "GLOBAL_DATA_PATH = os.path.join(DATA_PATH, \"globalData.json\")\n",
    "\n",
    "if refetch:\n",
    "    global_data = run_query(UNISWAP_V3_SUBGRAPH_URL, CURRENT_GLOBAL_DATA_QUERY)[\"data\"][\n",
    "        \"factory\"\n",
    "    ]\n",
    "    with open(GLOBAL_DATA_PATH, \"w\") as f:\n",
    "        json.dump(global_data, f, indent=4)\n",
    "else:\n",
    "    with open(GLOBAL_DATA_PATH, \"r\") as f:\n",
    "        global_data = json.load(f)\n",
    "\n",
    "pprint(global_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 1000 tokens by TVL (but only analyze top 30)\n",
    "TOP_TOKENS_QUERY = \"\"\"\n",
    "{\n",
    "    tokens(first: 1000, orderBy: totalValueLockedUSD, orderDirection: desc) {\n",
    "        id\n",
    "        symbol\n",
    "        name\n",
    "        totalValueLockedUSD\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "TOKENS_DF_PATH = os.path.join(DATA_PATH, \"tokens_df.csv\")\n",
    "\n",
    "if refetch:\n",
    "    top_tokens = run_query(UNISWAP_V3_SUBGRAPH_URL, TOP_TOKENS_QUERY)[\"data\"]\n",
    "    tokens_df = pd.DataFrame.from_dict(top_tokens[\"tokens\"])\n",
    "    tokens_df.to_csv(TOKENS_DF_PATH, index=False)\n",
    "else:\n",
    "    tokens_df = pd.read_csv(TOKENS_DF_PATH)\n",
    "\n",
    "# set column dtype\n",
    "tokens_df = tokens_df.astype(\n",
    "    {\"id\": str, \"symbol\": str, \"name\": str, \"totalValueLockedUSD\": np.float64}\n",
    ")\n",
    "\n",
    "tokens_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Day Time Series\n",
    "Ref: https://github.com/Uniswap/v3-info/blob/770a05dc1a191cf229432ebc43c1f2ceb3666e3b/src/data/tokens/chartData.ts#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_key(symbol: str, addr: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a key for a token.\n",
    "    The uniqueness of the key is guaranteed by the address,\n",
    "    but the symbol is also prefixed for readability.\n",
    "    \"\"\"\n",
    "    return f\"{symbol}_{addr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DAY_TIME_SERIES = \"\"\"\n",
    "    query tokenDayDatas($startTime: Int!, $skip: Int!, $address: String!) {\n",
    "        tokenDayDatas(\n",
    "            first: 1000\n",
    "            skip: $skip\n",
    "            where: { token: $address, date_gt: $startTime }\n",
    "            orderBy: date\n",
    "            orderDirection: asc\n",
    "            subgraphError: allow\n",
    "        ) {\n",
    "            date\n",
    "            volumeUSD\n",
    "            totalValueLockedUSD\n",
    "        }\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_token_error = []\n",
    "\n",
    "\n",
    "async def fetch_token_chart_data(\n",
    "    address: str, symbol: str, transport: AIOHTTPTransport = transport\n",
    "):\n",
    "    error = False\n",
    "    skip = 0\n",
    "    all_found = False\n",
    "    result = {\"tokenDayDatas\": []}\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "        execute_timeout=8,  # TODO: investigate timeout\n",
    "    ) as session:\n",
    "        params = {\"address\": address, \"startTime\": START_TIMESTAMP, \"skip\": skip}\n",
    "        try:\n",
    "            while not all_found:\n",
    "                temp = await session.execute(\n",
    "                    gql(TOKEN_DAY_TIME_SERIES), variable_values=params\n",
    "                )\n",
    "                skip += 1000\n",
    "                if len(temp[\"tokenDayDatas\"]) < 1000 or error:\n",
    "                    all_found = True\n",
    "                if temp:\n",
    "                    # concat the lists\n",
    "                    result[\"tokenDayDatas\"] = (\n",
    "                        result[\"tokenDayDatas\"] + temp[\"tokenDayDatas\"]\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error = True\n",
    "            fetch_token_error.append(address)\n",
    "\n",
    "    if not error:\n",
    "        if not os.path.exists(DATA_TOKEN_DAY_PATH):\n",
    "            os.makedirs(DATA_TOKEN_DAY_PATH)\n",
    "\n",
    "        with open(\n",
    "            f\"{DATA_TOKEN_DAY_PATH}/{get_token_key(symbol, address)}.json\",\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_token_error = []\n",
    "\n",
    "if refetch:\n",
    "    # remove existing content in the out folder\n",
    "    for f in glob.glob(DATA_TOKEN_DAY_PATH + \"/*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    # fetch token day data for each token\n",
    "    for row in tokens_df.iloc[:30].itertuples():\n",
    "        await fetch_token_chart_data(\n",
    "            row.id,\n",
    "            row.symbol,\n",
    "        )\n",
    "    print(fetch_token_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_key(symbol0: str, symbol1: str, fee_tier: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates a key for a pool.\n",
    "    `token0`, `token1` and `feeTier` together uniquely define a pool.\n",
    "    But using symbol instead of token address involve a risk.\n",
    "    \"\"\"\n",
    "    return f\"{symbol0}_{symbol1}_{fee_tier}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 50 pools by TVL (but only analyze top 20)\n",
    "TOP_POOLS_QUERY = \"\"\"\n",
    "{\n",
    "    pools(first: 50, orderBy: totalValueLockedUSD, orderDirection: desc) {\n",
    "        id\n",
    "        token0 {\n",
    "            id\n",
    "            symbol\n",
    "        }\n",
    "        token1 {\n",
    "            id\n",
    "            symbol\n",
    "        }\n",
    "        feeTier\n",
    "        totalValueLockedUSD\n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "POOLS_DF_PATH = os.path.join(DATA_PATH, \"pools_df.csv\")\n",
    "\n",
    "if refetch:\n",
    "    top_pools = run_query(UNISWAP_V3_SUBGRAPH_URL, TOP_POOLS_QUERY)[\"data\"]\n",
    "    pools_df = pd.json_normalize(top_pools[\"pools\"])\n",
    "    pools_df.to_csv(POOLS_DF_PATH, index=False)\n",
    "else:\n",
    "    pools_df = pd.read_csv(POOLS_DF_PATH)\n",
    "\n",
    "# set column dtype\n",
    "pools_df = pools_df.astype(\n",
    "    {\n",
    "        \"id\": str,\n",
    "        \"feeTier\": int,\n",
    "        \"totalValueLockedUSD\": np.float64,\n",
    "        \"token0.id\": str,\n",
    "        \"token0.symbol\": str,\n",
    "        \"token1.id\": str,\n",
    "        \"token1.symbol\": str,\n",
    "    }\n",
    ")\n",
    "\n",
    "# only analyze top 20\n",
    "pools_df = pools_df.iloc[:20]\n",
    "\n",
    "pools_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitelist a pool if both its token0 and token1 are whitelisted\n",
    "is_whitelisted_pool = pools_df[\"token0.id\"].isin(whitelisted_addresses) & pools_df[\n",
    "    \"token1.id\"\n",
    "].isin(whitelisted_addresses)\n",
    "pools_df = pools_df[is_whitelisted_pool]\n",
    "\n",
    "# add name\n",
    "pools_df[\"name\"] = pools_df.apply(\n",
    "    lambda x: get_pool_key(\n",
    "        addr2sym(x[\"token0.id\"]), addr2sym(x[\"token1.id\"]), x[\"feeTier\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "pools_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool Day Time Series\n",
    "Ref: https://github.com/Uniswap/v3-info/blob/770a05dc1a191cf229432ebc43c1f2ceb3666e3b/src/data/pools/chartData.ts#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_DAY_TIME_SERIES = \"\"\"\n",
    "    query poolDayDatas($startTime: Int!, $skip: Int!, $address: String!) {\n",
    "        poolDayDatas(\n",
    "            first: 1000\n",
    "            skip: $skip\n",
    "            where: { pool: $address, date_gt: $startTime }\n",
    "            orderBy: date\n",
    "            orderDirection: asc\n",
    "            subgraphError: allow\n",
    "        ) {\n",
    "            date\n",
    "            volumeUSD\n",
    "            tvlUSD\n",
    "        }\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_pool_error = []\n",
    "\n",
    "\n",
    "async def fetch_pool_chart_data(\n",
    "    address: str,\n",
    "    symbol0: str,\n",
    "    symbol1: str,\n",
    "    fee_tier: int,\n",
    "    transport: AIOHTTPTransport = transport,\n",
    "):\n",
    "    error = False\n",
    "    skip = 0\n",
    "    all_found = False\n",
    "    result = {\"poolDayDatas\": []}\n",
    "\n",
    "    async with Client(\n",
    "        transport=transport,\n",
    "        fetch_schema_from_transport=True,\n",
    "        execute_timeout=8,  # TODO: investigate timeout\n",
    "    ) as session:\n",
    "        params = {\"address\": address, \"startTime\": START_TIMESTAMP, \"skip\": skip}\n",
    "        try:\n",
    "            while not all_found:\n",
    "                temp = await session.execute(\n",
    "                    gql(POOL_DAY_TIME_SERIES), variable_values=params\n",
    "                )\n",
    "                skip += 1000\n",
    "                if len(temp[\"poolDayDatas\"]) < 1000 or error:\n",
    "                    all_found = True\n",
    "                if temp:\n",
    "                    # concat the lists\n",
    "                    result[\"poolDayDatas\"] = (\n",
    "                        result[\"poolDayDatas\"] + temp[\"poolDayDatas\"]\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error = True\n",
    "            fetch_pool_error.append(address)\n",
    "\n",
    "    if not error:\n",
    "        if not os.path.exists(DATA_POOL_DAY_PATH):\n",
    "            os.makedirs(DATA_POOL_DAY_PATH)\n",
    "\n",
    "        with open(\n",
    "            f\"{DATA_POOL_DAY_PATH}/{get_pool_key(symbol0, symbol1, fee_tier)}.json\",\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refetch:\n",
    "    # remove existing content in the out folder\n",
    "    for f in glob.glob(DATA_POOL_DAY_PATH + \"/*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    # fetch pool data for each pool\n",
    "    for i, row in pools_df.iterrows():\n",
    "        await fetch_pool_chart_data(\n",
    "            row[\"id\"],\n",
    "            row[\"token0.symbol\"],\n",
    "            row[\"token1.symbol\"],\n",
    "            row[\"feeTier\"],\n",
    "        )\n",
    "    print(fetch_pool_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads pool day datas from json\n",
    "pool_day_df = pd.DataFrame(columns=[\"date\"])\n",
    "pool_names = []\n",
    "\n",
    "for f in os.listdir(DATA_POOL_DAY_PATH):\n",
    "    fullname = os.fsdecode(f)\n",
    "\n",
    "    # not a rigorous check\n",
    "    with open(os.path.join(DATA_POOL_DAY_PATH, fullname), \"r\") as file:\n",
    "        pool_day_datas = json.load(file)\n",
    "\n",
    "    # parse dict as df\n",
    "    temp = pd.DataFrame.from_dict(pool_day_datas[\"poolDayDatas\"]).astype(\n",
    "        {\"volumeUSD\": np.float64, \"tvlUSD\": np.float64}\n",
    "    )\n",
    "\n",
    "    # prefix columns (except \"date\") with pool name\n",
    "    cols = temp.columns[~temp.columns.isin([\"date\"])]\n",
    "    pool_name = fullname.split(os.sep)[-1].split(\".\")[0]\n",
    "    pool_names.append(pool_name)\n",
    "    temp.rename(columns=dict(zip(cols, pool_name + \"_\" + cols)), inplace=True)\n",
    "\n",
    "    # outer join: union of items on \"date\"\n",
    "    pool_day_df = pd.merge(pool_day_df, temp, how=\"outer\", on=[\"date\"])\n",
    "\n",
    "# sort by \"date\"\n",
    "pool_day_df = pool_day_df.sort_values(by=\"date\").reset_index(drop=\"index\")\n",
    "\n",
    "pool_day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"date\"]: int -> date (in \"YYYY-MM-DD\")\n",
    "pool_day_df[\"timestamp\"] = pool_day_df[\"date\"]  # keep timestamp in a new col\n",
    "pool_day_df[\"date\"] = pool_day_df[\"date\"].map(dt.date.fromtimestamp)\n",
    "\n",
    "pool_day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_day_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for number of days elapsed\n",
    "d0 = pool_day_df[\"date\"].iloc[0]\n",
    "d1 = pool_day_df[\"date\"].iloc[-1]\n",
    "print(f\"{d0} to {d1} has {(d1 - d0).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_day_df.to_csv(os.path.join(DATA_PATH, \"pool_day_df.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('eth-uniswap-prelim-analysis-3WCyaTCY-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c4b7809061330062f8c735d3b8d55bf6a8b663fb533c57ef3c9775113b0f4a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
